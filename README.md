# ğŸ“¦ Smart Attrition Prediction â€“ Module 3: Attrition Risk Modeling

## ğŸ“Œ Overview

This module focuses on building an interpretable, high-performance machine learning model to predict employee attrition using HR data. It is part of the larger project **"Smart Attrition Prediction & Employee Wellness Recommendation Engine"**.

We apply classification techniques with resampling, hyperparameter tuning, and explainability to help HR teams reduce turnover and retain key talent.

---

## ğŸŒŸ Objectives

* Predict the likelihood of employee attrition
* Minimize false alarms (high precision)
* Capture actual attrition cases (high recall)
* Interpret model predictions with SHAP and feature importance

---

## ğŸ› ï¸ Tools & Technologies

* **Languages**: Python (Pandas, NumPy)
* **Modeling**: scikit-learn, XGBoost
* **Imbalance Handling**: SMOTE (imbalanced-learn)
* **Model Explainability**: SHAP
* **Visualization**: Matplotlib, Seaborn

---
## ğŸš€ Features

- ğŸ”„ **Handles class imbalance** using SMOTE and weighted models
- ğŸ“Š **Models used**: Logistic Regression, Random Forest, XGBoost
- ğŸ“ˆ **Evaluation**: Confusion matrix, ROC-AUC, Precision, Recall
- ğŸ§  **Explainability**: SHAP used to understand feature importance
- ğŸ¯ **Best Accuracy Achieved**: ~84% (XGBoost)

---
## ğŸ“ Project Structure

```
Smart_Attrition_Prediction_Module_3/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ Attrition_Model_Development.ipynb
â”œâ”€â”€ data/
â”‚ â””â”€â”€ M3_Cleaned_Final.csv
â”œâ”€â”€ models/
â”‚ â””â”€â”€ xgb_model.pkl
â”œâ”€â”€ predict.py
â”œâ”€â”€ shap_plot.py (optional)
â”œâ”€â”€ visuals/
â”œâ”€â”€ README.md
```

---

## ğŸ” Key Steps

### âœ… 1. Feature Selection

* Removed noisy or highly correlated features (VIF + EDA)
* Final feature set: 35 columns selected from 60+ after analysis

### âœ… 2. Model Training

* Models: Logistic Regression, Random Forest, XGBoost
* Resampling with SMOTE
* Stratified 5-fold Cross-validation
* Used class weights where applicable

### âœ… 3. Evaluation

* **Metrics**: Precision, Recall, F1 Score (not just accuracy)
* **Best Model**: XGBoost (after tuning)
* Hyperparameter tuning with GridSearchCV

### âœ… 4. Interpretability

* Feature Importance (Random Forest and XGBoost)
* SHAP Values for local + global explanation

---

## ğŸ“Š Model Comparison

| Model               | Accuracy | Precision | Recall | F1-Score |
| ------------------- | -------- | --------- | ------ | -------- |
| Logistic Regression | 0.72     | 0.33      | 0.72   | 0.46     |
| Random Forest       | 0.84     | 0.49      | 0.45   | 0.47     |
| **XGBoost (Best)**  | 0.84     | 0.50      | 0.43   | 0.46     |

---

## ğŸ“ˆ Top Features Driving Attrition

### ğŸ”¹ Random Forest

* YearsAtCompany
* Age
* MonthlyIncome

### ğŸ”¹ XGBoost

* OverTime\_Yes
* SatisfactionIndex
* YearsAtCompany

### ğŸ§  SHAP Insights:

* Employees with **low satisfaction + frequent overtime** are at highest attrition risk.

---

## ğŸ“„ Deliverables

\- \[x] âœ”ï¸ Cleaned & Feature-Selected Dataset

\- \[x] âœ”ï¸ Trained Models with Cross-validation

\- \[x] âœ”ï¸ SMOTE & Hyperparameter Tuning

\- \[x] âœ”ï¸ SHAP Value Analysis

\- \[x] âœ”ï¸ Business Report: Top 3 Drivers of Attrition

\- \[x] âœ”ï¸ Visualizations & Performance Metrics

---

## ğŸ“Œ Business Insights Summary

> The top drivers of attrition are **Overtime**, **Low Job Satisfaction**, and **Short Tenure**. Targeted HR interventions (e.g., work-life balance programs, incentive structure, career pathing) can reduce early exits.

---

## ğŸš€ How to Run

1. Clone the repo
   `git clone https://github.com/JPragna/-Smart_Attrition_Prediction_Module_3.git`

2. Install dependencies
   `pip install -r requirements.txt`

3. Open and run `notebooks/Attrition_Model_Development.ipynb`
   (Ensure Jupyter/Colab is set up)

---
or 
## ğŸ› ï¸ How to Use

### 1. ğŸ“Š Train the Model
Run the notebook:
notebooks/Attrition_Model_Development.ipynb

This trains the models and outputs performance metrics + SHAP plots.

### 2. ğŸ”® Make Predictions
After training, run:
python predict.py
Ensure your new input data is placed in:

bash
Copy code
data/new_employee_data.csv

## ğŸ“Š Model Evaluation

To assess and compare model performance, we evaluated the models using confusion matrices and ROC curves:

### âœ… Evaluated Models:

* Logistic Regression
* Random Forest
* XGBoost

### ğŸ“Š Evaluation Visuals:

| Model               | Evaluation Image |
| ------------------- | ---------------- |
| Logistic Regression |                  |
| Random Forest       |                  |
| XGBoost             |                  |

> ğŸ“ All visual outputs are in the `model_evaluation_visual/` folder.

### ğŸ§  Key Insight

> Ensemble models like Random Forest and XGBoost showed stronger precision and recall. SHAP confirms that overtime and job dissatisfaction are major attrition indicators.

---

## ğŸ‘©â€ğŸ’¼ Author

**Pragna J**
Data Science Intern | AI Enthusiast

---

## â­ï¸ If you like this repo, don't forget to star it!
